{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a09ac9c8-561b-4e52-afc0-845b4d1ef3ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sun Oct 29 01:02:28 2023       \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 535.113.01             Driver Version: 535.113.01   CUDA Version: 12.2     |\n",
      "|-----------------------------------------+----------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                      |               MIG M. |\n",
      "|=========================================+======================+======================|\n",
      "|   0  NVIDIA GeForce RTX 3060 Ti     Off | 00000000:01:00.0  On |                  N/A |\n",
      "|  0%   32C    P8              19W / 225W |    530MiB /  8192MiB |      0%      Default |\n",
      "|                                         |                      |                  N/A |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "|   1  NVIDIA GeForce GTX 1080        Off | 00000000:04:00.0 Off |                  N/A |\n",
      "|  0%   22C    P8               7W / 215W |      6MiB /  8192MiB |      0%      Default |\n",
      "|                                         |                      |                  N/A |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "                                                                                         \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                            |\n",
      "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
      "|        ID   ID                                                             Usage      |\n",
      "|=======================================================================================|\n",
      "|    0   N/A  N/A      1606      G   /usr/lib/xorg/Xorg                          307MiB |\n",
      "|    0   N/A  N/A      2068      G   /usr/bin/gnome-shell                         76MiB |\n",
      "|    0   N/A  N/A    878620      G   /usr/lib/firefox/firefox                    136MiB |\n",
      "|    1   N/A  N/A      1606      G   /usr/lib/xorg/Xorg                            4MiB |\n",
      "+---------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!/usr/bin/nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c2e09104-a30c-4b5a-ad0a-0a1e7365f4ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "sampleMediaDir = \"/mnt/m2media/sampleMedia\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8691f2d0-a111-4335-952c-e4e4bcab04ef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/mnt/m2media/sampleMedia'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sampleMediaDir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c20479b3-62d5-4d86-aa31-f84dc23727f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " EAH1.mp4\n",
      "'How Did The Sriracha Shortage Happen_.mp4'\n",
      "'How Did The Sriracha Shortage Happen_.wav'\n"
     ]
    }
   ],
   "source": [
    "!ls $sampleMediaDir"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d47a623-2464-4589-a8c3-cd40780509f7",
   "metadata": {},
   "source": [
    "## Set sample file to be used as audio data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1de8d811-4b1b-4c9c-bb30-105b6c62f1f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "audio_data = sampleMediaDir + \"/EAH1.mp4\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "85d7b340-0aeb-40e2-a207-c747bf79a98b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/mnt/m2media/sampleMedia/EAH1.mp4'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "audio_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95255d7a-1df5-44ff-af56-ff3e19c9d0b5",
   "metadata": {},
   "source": [
    "## check which Python interpreter you're using and the list of installed packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4a32c2cc-7ae6-4d1b-b8f3-20127949d223",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/winadmin/miniconda3/envs/periferyNABNY2023py38/bin/python\n",
      "\u001b[33mWARNING: No metadata found in /home/winadmin/miniconda3/envs/periferyNABNY2023py38/lib/python3.8/site-packages\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: No metadata found in /home/winadmin/miniconda3/envs/periferyNABNY2023py38/lib/python3.8/site-packages\u001b[0m\u001b[33m\n",
      "\u001b[0mopenai                        0.28.1\n",
      "openai-whisper                20230918\n"
     ]
    }
   ],
   "source": [
    "!which python\n",
    "!pip list | grep open"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1aa55edd-c5cf-4abe-a944-cc7526cdac13",
   "metadata": {},
   "outputs": [],
   "source": [
    "import whisper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f8ed6d95-9608-4c55-8e0e-525443ec4413",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on package whisper:\n",
      "\n",
      "NAME\n",
      "    whisper\n",
      "\n",
      "PACKAGE CONTENTS\n",
      "    __main__\n",
      "    audio\n",
      "    decoding\n",
      "    model\n",
      "    normalizers (package)\n",
      "    timing\n",
      "    tokenizer\n",
      "    transcribe\n",
      "    triton_ops\n",
      "    utils\n",
      "    version\n",
      "\n",
      "FUNCTIONS\n",
      "    available_models() -> List[str]\n",
      "        Returns the names of available models\n",
      "    \n",
      "    load_model(name: str, device: Union[torch.device, str, NoneType] = None, download_root: str = None, in_memory: bool = False) -> whisper.model.Whisper\n",
      "        Load a Whisper ASR model\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        name : str\n",
      "            one of the official model names listed by `whisper.available_models()`, or\n",
      "            path to a model checkpoint containing the model dimensions and the model state_dict.\n",
      "        device : Union[str, torch.device]\n",
      "            the PyTorch device to put the model into\n",
      "        download_root: str\n",
      "            path to download the model files; by default, it uses \"~/.cache/whisper\"\n",
      "        in_memory: bool\n",
      "            whether to preload the model weights into host memory\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        model : Whisper\n",
      "            The Whisper ASR model instance\n",
      "\n",
      "DATA\n",
      "    List = typing.List\n",
      "    Optional = typing.Optional\n",
      "    Union = typing.Union\n",
      "\n",
      "VERSION\n",
      "    20230918\n",
      "\n",
      "FILE\n",
      "    /home/winadmin/miniconda3/envs/periferyNABNY2023py38/lib/python3.8/site-packages/whisper/__init__.py\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(whisper)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8568f0b-1da8-4a60-b411-c8138beb9dc2",
   "metadata": {},
   "source": [
    "# Import Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b0c403fc-3250-47b3-a9bf-915cd389061e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import whisper\n",
    "from whisper import load_model\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bb7cf07e-fb76-4882-86cd-cdf3df4179e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95fa29cb-b85d-4689-ad76-276c2077e4fb",
   "metadata": {},
   "source": [
    "# Check if torch is available"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d52f28c-7749-41e1-ba1e-1f6c74d41b34",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "print(torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33165f32-18fe-4d9c-9fbd-425190b532c4",
   "metadata": {},
   "source": [
    "# List available models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3938e52-046f-4ab9-8ed3-2d8ff596f0e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "available_models = whisper.available_models()\n",
    "print(available_models)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "691cce7f-33cb-4c9f-a480-0c7eb68e9c60",
   "metadata": {},
   "source": [
    "# Load a model (for this example, we assume the first available model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1ee39b9-2bfc-497f-8616-d4ad078ea788",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = available_models[0]\n",
    "asr_model = whisper.load_model(model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed09df8a-a36a-4d3b-9cd2-307345e2fec8",
   "metadata": {},
   "source": [
    "# Use the moviepy library to determine the duration of the source"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28c1abed-e065-46f6-92f1-e16a4c54bf17",
   "metadata": {},
   "outputs": [],
   "source": [
    "from moviepy.editor import AudioFileClip, VideoFileClip\n",
    "\n",
    "# Function to get duration\n",
    "def get_media_duration(audio_data):\n",
    "    if audio_data.endswith(('.mp4', '.mkv', '.webm', '.flv', '.mov', '.avi')):\n",
    "        clip = VideoFileClip(audio_data)\n",
    "    else:  # Assuming other extensions are audio files\n",
    "        clip = AudioFileClip(audio_data)\n",
    "    duration = clip.duration\n",
    "    clip.close()\n",
    "    return duration\n",
    "\n",
    "# Get and print media duration\n",
    "duration = get_media_duration(audio_data)  # Replace with the path to your audio_data\n",
    "print(f\"Media Duration: {duration} seconds\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f08f7f05-40dc-4101-a1f7-017dd0346966",
   "metadata": {},
   "source": [
    "# Transcribe Audio and calculate the time taken:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fed264a8-a569-4c29-bf08-c504baaa0577",
   "metadata": {},
   "source": [
    "### (Assuming audio_data is the audio file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6548f1ef-0766-42a8-bff0-8301275b2ee3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "start_time = time.time()\n",
    "transcription = asr_model.transcribe(audio_data)\n",
    "end_time = time.time()\n",
    "\n",
    "processing_time = end_time - start_time\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "478f4b2f-9013-43d7-a8c9-78399174b288",
   "metadata": {},
   "source": [
    "## calculate and print the performance ratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f745b86-edc4-422d-9e6e-d8436b0ab696",
   "metadata": {},
   "outputs": [],
   "source": [
    "performance_ratio = duration / processing_time\n",
    "print(f\"Media Duration: {duration:.2f} seconds | Total Processing Time = {processing_time:.2f} seconds | Performance: {performance_ratio:.2f}x real-time\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53924bb0-2bee-40c5-a3da-80f546d9e0f6",
   "metadata": {},
   "source": [
    "## Print the Transcription"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b1faba6-3de6-41d2-88dc-69607c019dd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(transcription)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a345cc54-dd8d-402e-9349-39224ad38382",
   "metadata": {},
   "source": [
    "# Generate Report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5fbb75a-513d-4af3-8b33-0578d42cd248",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import csv\n",
    "import torch\n",
    "\n",
    "# Function to gather performance metrics (Placeholder, replace with actual metric gathering code)\n",
    "def gather_metrics():\n",
    "    # Replace this with code that gathers actual performance metrics\n",
    "    return {\n",
    "        \"gpu_name\": torch.cuda.get_device_name(0) if torch.cuda.is_available() else \"CPU\",\n",
    "        \"whisper_model\": \"Placeholder\",\n",
    "        \"performance_metric\": \"Placeholder\"\n",
    "    }\n",
    "\n",
    "# Create or open CSV file to store performance metrics\n",
    "csv_file_path = \"gpu_performance_metrics.csv\"\n",
    "file_exists = os.path.isfile(csv_file_path)\n",
    "with open(csv_file_path, 'a', newline='') as csvfile:\n",
    "    fieldnames = [\"gpu_name\", \"whisper_model\", \"performance_metric\"]\n",
    "    writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n",
    "    \n",
    "    # Write header only if the file didn't exist\n",
    "    if not file_exists:\n",
    "        writer.writeheader()\n",
    "\n",
    "    # Gather performance metrics and write to CSV\n",
    "    metrics = gather_metrics()\n",
    "    writer.writerow(metrics)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "perifery102723",
   "language": "python",
   "name": "periferynabny2023py38"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
