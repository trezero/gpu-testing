=== GPU: 535.113.01 ===

=== Initial GPU Status ===
Mon Oct 16 00:48:34 2023       
+---------------------------------------------------------------------------------------+
| NVIDIA-SMI 535.113.01             Driver Version: 535.113.01   CUDA Version: 12.2     |
|-----------------------------------------+----------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |
|                                         |                      |               MIG M. |
|=========================================+======================+======================|
|   0  NVIDIA GeForce GTX 1080        Off | 00000000:01:00.0  On |                  N/A |
|  0%   29C    P8              11W / 215W |    944MiB /  8192MiB |     10%      Default |
|                                         |                      |                  N/A |
+-----------------------------------------+----------------------+----------------------+
                                                                                         
+---------------------------------------------------------------------------------------+
| Processes:                                                                            |
|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |
|        ID   ID                                                             Usage      |
|=======================================================================================|
|    0   N/A  N/A      1456      G   /usr/lib/xorg/Xorg                          523MiB |
|    0   N/A  N/A      1904      G   /usr/bin/gnome-shell                         78MiB |
|    0   N/A  N/A     34197      G   /usr/lib/xorg/Xorg                          101MiB |
|    0   N/A  N/A     34758      G   ...05530350,7739799275644995955,262144      208MiB |
|    0   N/A  N/A     35104      G   ...,WinRetrieveSuggestionsOnlyOnDemand       16MiB |
+---------------------------------------------------------------------------------------+



Command Execution Time: 44.653560400009155 seconds

=== Post Execution GPU Status ===
Mon Oct 16 00:49:18 2023       
+---------------------------------------------------------------------------------------+
| NVIDIA-SMI 535.113.01             Driver Version: 535.113.01   CUDA Version: 12.2     |
|-----------------------------------------+----------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |
|                                         |                      |               MIG M. |
|=========================================+======================+======================|
|   0  NVIDIA GeForce GTX 1080        Off | 00000000:01:00.0  On |                  N/A |
|  0%   37C    P0              50W / 215W |    888MiB /  8192MiB |      0%      Default |
|                                         |                      |                  N/A |
+-----------------------------------------+----------------------+----------------------+
                                                                                         
+---------------------------------------------------------------------------------------+
| Processes:                                                                            |
|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |
|        ID   ID                                                             Usage      |
|=======================================================================================|
|    0   N/A  N/A      1456      G   /usr/lib/xorg/Xorg                          523MiB |
|    0   N/A  N/A      1904      G   /usr/bin/gnome-shell                         78MiB |
|    0   N/A  N/A     34197      G   /usr/lib/xorg/Xorg                          101MiB |
|    0   N/A  N/A     34758      G   ...05530350,7739799275644995955,262144      153MiB |
|    0   N/A  N/A     35104      G   ...,WinRetrieveSuggestionsOnlyOnDemand       16MiB |
+---------------------------------------------------------------------------------------+

=== Errors ===
⚠️ WARNING | 2023-10-16 00:48:35 | autotrain.cli.run_dreambooth:<module>:14 - ❌ Some DreamBooth components are missing! Please run `autotrain setup` to install it. Ignore this warning if you are not using DreamBooth or running `autotrain setup` already.
> INFO    Running LLM
> INFO    Params: Namespace(version=False, train=True, deploy=False, inference=False, data_path='.', train_split='train', valid_split=None, text_column='text', rejected_text_column='rejected', model='mistralai/Mistral-7B-Instruct-v0.1', learning_rate=0.0002, num_train_epochs=3, train_batch_size=6, warmup_ratio=0.1, gradient_accumulation_steps=1, optimizer='adamw_torch', scheduler='linear', weight_decay=0.0, max_grad_norm=1.0, seed=42, add_eos_token=False, block_size=-1, use_peft=True, lora_r=16, lora_alpha=32, lora_dropout=0.05, logging_steps=-1, project_name='my-llm', evaluation_strategy='epoch', save_total_limit=1, save_strategy='epoch', auto_find_batch_size=False, fp16=False, push_to_hub=False, use_int8=False, model_max_length=1024, repo_id=None, use_int4=True, trainer='sft', target_modules='q_proj,v_proj', merge_adapter=False, token=None, backend='default', username=None, use_flash_attention_2=False, disable_gradient_checkpointing=False, func=<function run_llm_command_factory at 0x7f002f0eedd0>)
> INFO    loading dataset from csv
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Using pad_token, but it is not set yet.

Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]
Loading checkpoint shards:  50%|█████     | 1/2 [00:20<00:20, 20.31s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [00:31<00:00, 14.88s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [00:31<00:00, 15.69s/it]
> INFO    Using block size 1024
> INFO    creating trainer
/home/winadmin/miniconda3/envs/gpuTesting310/lib/python3.10/site-packages/trl/trainer/sft_trainer.py:214: UserWarning: You passed a tokenizer with `padding_side` not equal to `right` to the SFTTrainer. This might lead to some unexpected behaviour due to overflow issues when training a model in half-precision. You might consider adding `tokenizer.padding_side = 'right'` to your code.
  warnings.warn(

  0%|          | 0/9 [00:00<?, ?it/s]You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
/home/winadmin/miniconda3/envs/gpuTesting310/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
> ERROR   train has failed due to an exception:
> ERROR   Traceback (most recent call last):
  File "/home/winadmin/miniconda3/envs/gpuTesting310/lib/python3.10/site-packages/autotrain/utils.py", line 280, in wrapper
    return func(*args, **kwargs)
  File "/home/winadmin/miniconda3/envs/gpuTesting310/lib/python3.10/site-packages/autotrain/trainers/clm/__main__.py", line 396, in train
    trainer.train()
  File "/home/winadmin/miniconda3/envs/gpuTesting310/lib/python3.10/site-packages/transformers/trainer.py", line 1591, in train
    return inner_training_loop(
  File "/home/winadmin/miniconda3/envs/gpuTesting310/lib/python3.10/site-packages/transformers/trainer.py", line 1892, in _inner_training_loop
    tr_loss_step = self.training_step(model, inputs)
  File "/home/winadmin/miniconda3/envs/gpuTesting310/lib/python3.10/site-packages/transformers/trainer.py", line 2787, in training_step
    self.accelerator.backward(loss)
  File "/home/winadmin/miniconda3/envs/gpuTesting310/lib/python3.10/site-packages/accelerate/accelerator.py", line 1983, in backward
    self.scaler.scale(loss).backward(**kwargs)
  File "/home/winadmin/miniconda3/envs/gpuTesting310/lib/python3.10/site-packages/torch/_tensor.py", line 492, in backward
    torch.autograd.backward(
  File "/home/winadmin/miniconda3/envs/gpuTesting310/lib/python3.10/site-packages/torch/autograd/__init__.py", line 251, in backward
    Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
  File "/home/winadmin/miniconda3/envs/gpuTesting310/lib/python3.10/site-packages/torch/autograd/function.py", line 288, in apply
    return user_fn(self, *args)
  File "/home/winadmin/miniconda3/envs/gpuTesting310/lib/python3.10/site-packages/torch/utils/checkpoint.py", line 271, in backward
    outputs = ctx.run_function(*detached_inputs)
  File "/home/winadmin/miniconda3/envs/gpuTesting310/lib/python3.10/site-packages/transformers/models/mistral/modeling_mistral.py", line 921, in custom_forward
    return module(*inputs, past_key_value, output_attentions, padding_mask=padding_mask)
  File "/home/winadmin/miniconda3/envs/gpuTesting310/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/winadmin/miniconda3/envs/gpuTesting310/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/winadmin/miniconda3/envs/gpuTesting310/lib/python3.10/site-packages/accelerate/hooks.py", line 165, in new_forward
    output = old_forward(*args, **kwargs)
  File "/home/winadmin/miniconda3/envs/gpuTesting310/lib/python3.10/site-packages/transformers/models/mistral/modeling_mistral.py", line 635, in forward
    hidden_states = self.mlp(hidden_states)
  File "/home/winadmin/miniconda3/envs/gpuTesting310/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/winadmin/miniconda3/envs/gpuTesting310/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/winadmin/miniconda3/envs/gpuTesting310/lib/python3.10/site-packages/accelerate/hooks.py", line 165, in new_forward
    output = old_forward(*args, **kwargs)
  File "/home/winadmin/miniconda3/envs/gpuTesting310/lib/python3.10/site-packages/transformers/models/mistral/modeling_mistral.py", line 198, in forward
    return self.down_proj(self.act_fn(self.gate_proj(x)) * self.up_proj(x))
  File "/home/winadmin/miniconda3/envs/gpuTesting310/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/winadmin/miniconda3/envs/gpuTesting310/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/winadmin/miniconda3/envs/gpuTesting310/lib/python3.10/site-packages/accelerate/hooks.py", line 165, in new_forward
    output = old_forward(*args, **kwargs)
  File "/home/winadmin/miniconda3/envs/gpuTesting310/lib/python3.10/site-packages/bitsandbytes/nn/modules.py", line 248, in forward
    out = bnb.matmul_4bit(x, self.weight.t(), bias=bias, quant_state=self.weight.quant_state)
  File "/home/winadmin/miniconda3/envs/gpuTesting310/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py", line 579, in matmul_4bit
    return MatMul4Bit.apply(A, B, out, bias, quant_state)
  File "/home/winadmin/miniconda3/envs/gpuTesting310/lib/python3.10/site-packages/torch/autograd/function.py", line 539, in apply
    return super().apply(*args, **kwargs)  # type: ignore[misc]
  File "/home/winadmin/miniconda3/envs/gpuTesting310/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py", line 516, in forward
    output = torch.nn.functional.linear(A, F.dequantize_4bit(B, state).to(A.dtype).t(), bias)
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 224.00 MiB. GPU 0 has a total capacty of 7.92 GiB of which 164.25 MiB is free. Including non-PyTorch memory, this process has 6.89 GiB memory in use. Of the allocated memory 6.37 GiB is allocated by PyTorch, and 397.10 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF


  0%|          | 0/9 [00:03<?, ?it/s]
